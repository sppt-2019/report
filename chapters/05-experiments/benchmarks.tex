\section{Benchmarks}\label{sec:benchmarks}
In this section we explore the performance of the lenient evaluation strategy (see \secref{implicit-para}) and how well it parallelises. The reason for our study is that interest in the lenient evaluation strategy has been lacking to say the least. We could not find a reason as to why and decided to explore whether it was because \cite{DBLP:journals/cl/Tremblay-parallel} gave false promises of the high implicit parallelisability of leniently evaluated programs.

\subsection{Test cases}
\cite{DBLP:journals/cl/Tremblay-parallel} presents two test cases that we reuse in this experiment:

\begin{labeling}{\quad\quad}
    \item[Binary Tree Sum] where a tree-walker sums the values of all leaves of a binary tree.
    \item[Binary Tree Accumulation and Sort] where a tree-walker flattens all values of a tree's leaves into a list and sorts it.
\end{labeling}

The difference between the two test cases is that there are no data-dependencies in the summation test, i.e. we can calculate the results of the left subtree and the right subtree in parallel. In the Binary Tree Accumulation and Sort benchmark the tree must be traversed in a left-to-right order and thus there is a data-dependency between the results from the left and right subtree.

\subsection{Implementations}\label{sec:bench-impls}
We implemented the benchmark suite in both F\# and C\#. In this test F\# will be the primary test subject, as we wish to examine how well it parallelises. The C\# implementation will be used as control, in the sense that we will compare, for the different strategies, which of the languages is fastest. We are well aware that F\# is not lenient, but we wanted to compare the strategies within the same language runtime and therefore attempted to map the lenient evaluation strategy onto F\# using its async workflows. \lstref{lenient:to:task} gives a code example how to do so (a similar mapping could also be made to .NET's \m{Task}s).

\begin{listing}[H]
\begin{minted}{fsharp}
//Lenient function, body evaluated asynchronously from caller
//parameters evaluated asynchronously from function body
let CalculateC a b =
    let c = a * 50 //Implicit synchronisation with a-thread
    c + b;   //Implicit synchronisation with b-thread

//F# lenient mapping of the CalculateC-function
//async function that runs asynchronously from the caller
let CalculateCLenient a b = async {
    let! aResult = a
    let c = aResult * 50
    let! bResult = b
    return c + b
}
\end{minted}
\caption{Lenient evaluation mapping using F\# Async Workflows.} \label{lst:lenient:to:task}
\end{listing}
The mapping shown in \lstref{lenient:to:task} involves wrapping all arguments to methods in Async Workflows and explicitly synchronise using the \ttt{let!} keyword whenever there is a data-dependency between them. This means that all values of arguments are calculated asynchronously from the method body. The method is marked as \ttt{async}, meaning that an Async Workflow is spawned for every invocation of the method, i.e. it runs asynchronously from the caller. This leaves the majority of the footwork to F\#'s scheduler, which must figure out in which order to execute them.

We implemented the test cases in four different synchronization models:
\begin{labeling}{\quad\quad}
    \item[Sequential] which uses divide and conquer on one unit of execution to do all the calculations. This provides the baseline speed for the computation on a single core.
    \item[Async Workflows] which also uses divide and conquer, but in each recursion step, two Async Workflows are spawned to compute the results of the left and right subtree. The Workflows are then synchronised at the end of each recursive call.% This provides a \textit{\dquote{classic parallel}} baseline speed.
    \item[Task] which uses C\#'s \ttt{Task}-library and divide and conquer. It is similar to the previous, except that it spawns \ttt{Task}s.
    \item[Lenient] using the mapping displayed in \lstref{lenient:to:task}, i.e. wrapping everything in Async Workflows and have the .NET Core task scheduler figure out the computation order.
\end{labeling}

\subsection{Test Setup}
According to \cite{sestoft2013microbenchmarks} the most reliable results are obtained when the tests are repeated multiple times and the average and standard deviation calculated for the results. We therefore decided to take the average execution speed over 100 repetitions for varying problem sizes, starting with 1 leaf node and gradually doubling the number up to a total of 65536 nodes.

The tests were run on a laptop, of which the specifications are listed in \tableref{sys-specs}.

\makeTable{
{| l | R{6em} | p{3em} |}
\hline
\multicolumn{3}{| c |}{\textbf{Processor}} \\ \hline
Model & \multicolumn{2}{| c |}{Intel Core i7 4702HQ} \\ \hline
Clock Frequency & 2.2 & GHz \\ \hline
Max Turbo & 3.2 & GHz \\ \hline
Physical & 4 & Cores \\ \hline
Logical\footnotemark & 8 & Cores \\ \hline
\multicolumn{3}{|c|}{\textbf{Memory}} \\ \hline
Memory Size & 16 & GiB  \\ \hline
Memory Speed & 1600 & MHz \\ \hline
Memory Type &  \multicolumn{2}{| c |}{DDR3L 1600} \\ \hline
\multicolumn{3}{|c|}{\textbf{Software}} \\ \hline
Operating System & \multicolumn{2}{| c |}{Ubuntu 18.04 64bit}  \\ \hline
C\# runtime & \multicolumn{2}{| c |}{dotnet 2.2.104} \\ \hline
}{System specifications of the test machine.}{sys-specs}\footnotetext{Logical cores are sometimes called threads. However logical cores is used here to avoid confusion with the software concept; threads, which is distinct from hardware threads.}

We wish to analyse the following research questions:
\begin{enumerate}
    \item Which of the four presented strategies handle increasing sizes of trees best?
    \item Does F\# have worse performance than C\# when running equivalent (concurrent) code?
\end{enumerate}

\subsection{Results}\label{sec:bench-res}
In this section we analyse and discuss the results based on the two research questions presented in the previous section.

\subsubsection{Parallel Strategy and Performance}
The results are plotted in \figref{binary-accumulation} and \figref{binary-summation} (and listed in \tableref{accumulation:res} and \tableref{summation:res} in \apxref{benchmark:data}).

\newcommand{\binAcumSymbolics}{\symbolic{Problem,1,4,8,16,32,64,128,256,512,1024,2048,4096,8192,16384,32768,65536}}
\barChart*[5][\binAcumSymbolics][Run Time (ns)][Number of Nodes]{Binary accumulation benchmark results in F\# (lower is better).}{binary-accumulation}{
    \plotDataWithError{Sequential}{\accumulationData}
    \plotDataWithError{Async Workflow}{\accumulationData}
    \plotDataWithError{Task}{\accumulationData}
    \plotDataWithError{Lenient}{\accumulationData}
}
\barChart*[5][\binAcumSymbolics][Run Time (ns)][Number of Nodes]{Binary summation benchmark results in F\# (lower is better).}{binary-summation}{
    \plotDataWithError{Sequential}{\summationData}
    \plotDataWithError{Async Workflow}{\summationData}
    \plotDataWithError{Task}{\summationData}
    \plotDataWithError{Lenient}{\summationData}
}

Much to our surprise, the sequential implementation was actually the fastest in all cases. It seems that the overhead of spawning and synchronising \ttt{Task}s outweighs the performance gain of concurrency when the problem sizes are in the magnitude of additions and list appending. Furthermore, the Accumulation test case presented in \cite{DBLP:journals/cl/Tremblay-parallel} is a poor choice when it comes to parallelism, as it must traverse the tree in a left-to-right manner, meaning that the only things that can be executed in parallel are the recursive calls down the tree. We suspect that the advantages of parallel programming will be more prominent, as the amount of work in each Async Workflow or \m{Task} increases. We will research this hypothesis in greater depth in the following section.

The execution times of the lenient strategy grows faster with problem size than those of the task strategy. This means that the lenient approach handle increasing sizes of trees worse. It is, however, not as bad as Async Workflows. We have only obtained data for Async Workflows up to trees containing 256 nodes, as the running times grew so rapidly that it was not feasible to continue. Strangely enough, the lenient strategy scales much better. The only difference between the two implementations are that workflows are started with \ttt{Async.StartChild} in the lenient implementation and \ttt{Async.Parallel} in the other. These types of results showcase the fragility of parallel programs, where small and seemingly irrelevant details may have a huge impact on the performance.

\subsubsection{Language Performance with C\# Tasks}
\figref{seq-binary-accumulation} and \figref{seq-binary-summation} shows the running time of the sequential implementation in C\# and F\#. These results show that F\# is faster in the accumulation benchmark up to tree sizes of roughly 256 nodes. In the summation benchmark C\# is faster all the way. Both data sets seem to decrease in running time up to tree sizes of 128 nodes. This is strange, as it means that the implementation handles more computations in less time. We are unsure what causes this.

\barChart*[10][\binAcumSymbolics][Run Time (ns)][Number of Nodes]{Binary Accumulation in F\# and C\# using the sequential solutions (lower is better).}{seq-binary-accumulation}{
    \plotDataWithErrorAndLegend{Sequential}{\accumulationData}{F\#}
    \plotDataWithErrorAndLegend{Sequential}{\accumulationDataCsharp}{C\#}
}
\barChart*[10][\binAcumSymbolics][Run Time (ns)][Number of Nodes]{Binary Summation in F\# and C\# using the sequential solutions (lower is better).}{seq-binary-summation}{
    \plotDataWithErrorAndLegend{Sequential}{\summationData}{F\#}
    \plotDataWithErrorAndLegend{Sequential}{\summationDataCsharp}{C\#}
}

In \figref{task-binary-accumulation} and \figref{task-binary-summation} we have plotted the running times for F\# and C\# in the two test cases. Both the C\# and F\# implementation uses the Task strategy (i.e. divide-and-conquer that spawns two tasks in each recursion step). These results align with the previous in that F\# is fastest when there is a small number of nodes in the tree (up to 128 nodes in accumulation and 8 nodes in summation). After that point C\# is faster, and it seems that C\# handle increasing number of nodes better. The strange curve we observed in the sequential data sets are also present in C\# in the binary summation benchmark. Again, we're unsure what causes this behaviour.

\barChart*[10][\binAcumSymbolics][Run Time (ns)][Number of Nodes]{Binary Accumulation in F\# and C\# using \ttt{Task} parallelisation (lower is better).}{task-binary-accumulation}{
    \plotDataWithErrorAndLegend{Task}{\accumulationData}{F\#}
    \plotDataWithErrorAndLegend{Fork Join}{\accumulationDataCsharp}{C\#}
}
\barChart*[10][\binAcumSymbolics][Run Time (ns)][Number of Nodes]{Binary Summation in F\# and C\# using \ttt{Task} parallelisation (lower is better).}{task-binary-summation}{
    \plotDataWithErrorAndLegend{Task}{\summationData}{F\#}
    \plotDataWithErrorAndLegend{Fork Join}{\summationDataCsharp}{C\#}
}
