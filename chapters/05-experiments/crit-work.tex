\section{Parallel Overhead \& Performance}\label{sec:crit:work}
In this section we present results from an experiment that estimates how much work need to be done in a task to outweigh the performance penalty of \ttt{Task} synchronisation. We also implement a matrix summation benchmark to determine how different parallelisation strategies handle matrices of increasing sizes.

\subsection{Estimating Minimum Concurrent Workload}
In this experiment we estimate the minimum concurrent workload of C\#'s \ttt{Task}-system. By minimum concurrent workload we mean how much time each task must execute before it is worthwhile to spawn it, compared to a sequential solution. The reason for this exploration is that we found the sequential solution to be faster in the binary tree benchmarks presented in the previous section.

\subsubsection{Test Setup}
We use the Binary Tree Summation benchmark presented in the previous section with a minor modification: Every time the algorithm finds a \ttt{Node}, it busy-waits for a given amount of time to simulate work. The busy-wait was implemented with a loop, whose number of iterations is gradually halved until the sequential solution executes faster than the parallel. The hypothesis here is that the parallel solutions will be faster, because it is capable of busy-waiting multiple tasks at the same time.

We implemented the test in two variations, which are both listed in \lstref{benchmark:strategies}. The first variation emulates a data dependency between the wait and the results the subtrees, i.e. the wait is intended to emulate a computation that must be carried out after the results of both subtrees have been computed. The other variation emulates a situation where the left and right subtree can be computed in parallel with the wait, i.e. no data dependency between the delay and the subtrees. The tree has a total of 60 leaf nodes. In addition to the binary tree summation, we also implemented a N-ary tree summation in the same variations as that of binary.

\begin{listing}
    \begin{minted}{csharp}
public static int DoFakeWork(int workBias) {
    //Start the 'work bias' before blocking wait on child computation, i.e. we're waiting while the children
    //are computing
    var fakeSum = 0;
    for(var i = 0; i < workBias / 2; i++) {
        fakeSum += i;
    }
    for (var i = 0; i < workBias / 2; i++) {
        fakeSum -= i;
    }
    return fakeSum;
}

public static async Task<int> SumLeaves(Tree<int> tree, int workBias)
{
    if (tree is Leaf<int> leaf)
        return leaf.Value;

    var sums = tree.Children.Select(c => SumLeaves(c, workBias)).ToList();
    
#if !DELAY_DEPENDS_ON_LR
    var wb = Task.Run(() => DoFakeWork(workBias));
    await Task.WhenAll(sums);
    var fakeSum = await wb;
#else
    await Task.WhenAll(sums);
    var fakeSum = DoFakeWork(workBias);
#endif
    
    return sums.Sum(t => t.Result) + fakeSum;
}
    \end{minted}
    \caption{Implementation of the two different data dependency strategies with an N-ary tree. The strategy may be selected by either defining or undefining the \ttt{DELAY_DEPENDS_ON_LR} preprocessor flag.}
    \label{lst:benchmark:strategies}
\end{listing}

\subsubsection{Results}
The results are plotted in \figureref{crit-work-dep} and \figureref{crit-work-no-dep} (and listed in \tableref{binary:tree:with:bias:dependency} and \tableref{binary:tree:with:bias:no:dependency} in \apxref{crit:work:data}).

\newcommand{\workBiasSymbolics}{\symbolic{Work Bias (iterations),134217728,67108864,33554432,16777216,8388608,4194304,2097152,1048576,524288,262144,131072,65536,32768,16384,8192,4096,2048,1024}}
\lineChart{Critical workload with data dependency.}{crit-work-dep}{
    \plotData{Sequential}{\workWithDependencyData}
    \plotData{Fork Join}{\workWithDependencyData}
    \plotData{Lenient}{\workWithDependencyData}
}
\lineChart{Critical workload without data dependency.}{crit-work-no-dep}{
    \plotData{Sequential}{\workWithoutDependencyData}
    \plotData{Fork Join}{\workWithoutDependencyData}
    \plotData{Lenient}{\workWithoutDependencyData}
}
\lineChart{Critical workload with data dependency, N-ary tree.}{crit-work-dep-nary}{
    \plotData{Sequential}{\workWithDependencyDataNary}
    \plotData{Fork Join}{\workWithDependencyDataNary}
    \plotData{Lenient}{\workWithDependencyDataNary}
}
\lineChart{Critical workload with data dependency, N-ary tree.}{crit-work-no-dep-nary}{
    \plotData{Sequential}{\workWithoutDependencyDataNary}
    \plotData{Fork Join}{\workWithoutDependencyDataNary}
    \plotData{Lenient}{\workWithoutDependencyDataNary}
}

\makeTable{
    { c | c | c }
    & No data dependency & Data dependency \\\hline
    Binary & 1024 & 4096 \\
    N-ary & 2048 & 2048
}{Iterations of the busy-wait loop before the sequantial solution becomes the fastest.}{crit:work:iterations}

The number of iterations in the busy-wait loop before the sequential solution is faster than the concurrent is listed in \tableref{crit:work:iterations}. The graphs also underline that our hypothesis was correct. The parallel solutions grows slower than the sequential because they are capable of executing multiple busy-waits concurrently. Finally we notice that the lenient and fork join strategy lie very close in execution speed. This is a promising result for the lenient evaluation strategy, as it shows that it may be as fast as a traditional concurrency strategy.

Some conccurency models batch smaller jobs together to form larger jobs\needcite. Such batching may reduce the time spent context switching and thus increase the execution speed of the concurrent solutions. Such strategy is employed by Unity's C\# job system\cite{unity:csharp:job:system}.

\subsection{Matrix Summation}
In this section we execute a matrix summation benchmark. This benchmark measures the time it takes to sum all indices of a random $N x N$ matrix. This benchmark was implemented in different parallelisation strategies to explore how well they scale to increasing sizes of $N$:

\begin{labeling}{\quad\quad}
    \item[Sequential] utilises a double-nested for-loop to iterate over the matrix and sum the values. This benchmark provides a baseline value for running the computation on one thread.
    \item[Map Reduce] maps a function that sums each column over the matrix. The resulting list of column sums is then reduced to the overall sum of the matrix. In C\# we utilise the \gls{LINQ}-methods \ttt{Select}, \ttt{Sum} and \ttt{Aggregate}.
    \item[Parallel Foreach] uses a parallel loop to iterate over the columns of the matrix that may execute the summation of each column in parallel.
    \item[Tasks] is similar to parallel foreach, with the only exception that we manually spawn a \ttt{Task} that calculates the sum of each column.
\end{labeling}

We have not included a lenient-variation in this experiment, as an implementation in our C\# mapping would be largely equivalent to the Tasks-implementation (see \lstref{matrix-sum-csharp}). The most notable difference being that a lenient-evaluation strategy would most likely also construct the matrix in parallel with the summation. As the time it takes to construct a matrix is not included in the results here, this should have no effect on the validity of the results.
\begin{listing}
\begin{minted}{csharp}
public static async Task<long> SumTask(long[,] matrix)
{
    //Create an enumerable over the columns of the matrix
    var columns  = Enumerable.Range(0, matrix.GetLength(0));
    //Sum each column in parallel
    var sums = columns.Select(c => Task.Run(() =>
    {
        var sum = 0L;
        for(var i = 0; i < matrix.GetLength(1); i++)
        {
            sum = unchecked(sum + matrix[c, i]);
        }

        return sum;
    })).ToList();

    //Join the resuls and sum the sums of each column
    await Task.WhenAll(sums);
    return sums.SumUnchecked();
}
\end{minted}
\caption{Tasks implementation of Matrix Sum, largely equal to a lenient C\# mapping.} \label{lst:matrix-sum-csharp}
\end{listing}

As the matrices are of size $N x N$, they contain a total of $N^2$ elements with random values between \ttt{Int64.Minvalue} and \ttt{Int64.MaxValue}. When running the test with large matrices we found that the result would overflow, which throws an exception because C\# is a managed language. In order to avoid this, we used the \ttt{unchecked}-keyword, which disables bounds-checking on an integral arithmetic operation\cite{csharp:unchecked}. \cite{csharp:unchecked} states that using \ttt{unchecked} \textit{\dquote{might improve performance}}, compared to checked integral arithmetic operations.

\subsubsection{Results}
The results are plotted in \figureref{linpack-summation}. The first thing to notice is that Map Reduce seems to be roughly equal to the sequential in running time. This could indicate that the \ttt{Select}-method of C\#'s \gls{LINQ}, which was used to implement Map Reduce, does not parallelise its iterations. We will thus treat Map Reduce as a sequential solution for the rest of this result discussion.

\newcommand{\linpackSymbolics}{\symbolic{Problem Size,2,4,8,16,32,64,128,256,512,1024,2048,4096}}
\barChart*[7][\linpackSymbolics]{Matrix Summation}{linpack-summation}{
    \plotDataWithError{Sequential}{\linpackData}
    \plotDataWithError{Map Reduce}{\linpackData}
    \plotDataWithError{Parallel Foreach}{\linpackData}
    \plotDataWithError{Tasks}{\linpackData}
}
In general, the results from this experiment is in alignment with those of the previous, in that there is an initial overhead associated with concurrency. In this case, it seems the sequential and concurrent solutions evens out at job sizes of around 256 summations, after which point the concurrent solutions are faster.

After overcoming the initial overhead, the concurrent solutions handle increasing matrix sizes much better than their sequential counterparts. This is even more notable in \figureref{linpack-summation-line}, which plots the same data as a line and without logarithmic y-axis. As the matrix sizes continue to grow, it may be possible to split the columns in multiple separate tasks, possibly making the concurrent implementations faster yet.

\lineChart{Matrix Summation}{linpack-summation-line}{
    \plotData{Sequential}{\linpackData}
    \plotData{Map Reduce}{\linpackData}
    \plotData{Parallel Foreach}{\linpackData}
    \plotData{Tasks}{\linpackData}
}

\subsection{Discussion}
Our results show, first and foremost, that there is an initial overhead in concurrent programming. In order to speed up a computation the task sizes must exceed a certain threshold. Our results show that this threshold lies around the size of 1024-4096 iterations in a for-loop that increments a variable\tmc{Måske skal vi tælle nogle instruktioner her?}, but also that it varies with the type of problem. Furthermore, the results show that the concurrency of lenient evaluation strategy is similar to that of classic concurrency strategies, but that the choice of problems in \cite{DBLP:journals/cl/Tremblay-parallel} is not well suited to parallelisation. Finally C\#'s \ttt{Task}-model seems to handle the matrix summation problem well, after exceeding the threshold of roughly 256 summations in each column.

As part of the project we implemented a simple \gls{FRP} system in Unity, because this particular programming paradigm is well suited to gameplay programming in functional languages\needcite. We had initially decided that this experiment would use either lenient evaluation or Async Workflows under the hood to update \ttt{MonoBehaviour}s concurrently, but discovered too late that Unity uses a custom concurrency strategy called Unity C\# Job System\cite{unity:csharp:job:system} (see \secref{unity:concurrency}). Unity therefore does not allow \ttt{MonoBehaviour} updates from \ttt{Task}s and Async Workflows\cite{unity:async}. The resulting \gls{FRP} system therefore runs sequentially.