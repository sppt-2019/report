\section{Parallel Overhead \& Performance}\label{sec:crit:work}
In this section we present results from an experiment that estimates how much work need to be done in an async workflow or task to outweigh the performance penalty. We also implement a matrix summation benchmark to determine how different parallelisation strategies handle matrices of increasing sizes.

\subsection{Estimating Minimum Concurrent Workload}
In this experiment we estimate the minimum concurrent workload of .NET's \ttt{Task} system. By minimum concurrent workload we mean how much time each task must execute before it is worthwhile to spawn it, compared to a sequential solution. The reason for this exploration is that we found the sequential solution to be faster in the binary tree benchmarks presented in the previous section. In this case we use the C\# implementation, as it was the fastest\tmc{Dårlig argumentation...}.

\subsubsection{Test Setup}
We use the binary tree summation benchmark presented in the previous section with a minor modification: Every time the algorithm finds a \ttt{Node}, it busy-waits for a given amount of time to simulate work. The busy-wait was implemented with a loop, whose number of iterations is gradually halved until the sequential solution executes faster than the parallel. The hypothesis here is that the parallel solutions will be faster with higher iteration counts, because it is capable of busy-waiting multiple tasks at the same time.

We implemented the test in two variations, which are both listed in \lstref{benchmark:strategies}. The first variation emulates a data dependency between the wait and the results the subtrees, i.e. the wait is intended to emulate a computation that must be carried out after the results of both subtrees have been computed. The other variation emulates a situation where the left and right subtree can be computed in parallel with the wait, i.e. no data dependency between the delay and the subtrees. The tree has a total of 60 leaf nodes. In addition to the binary tree summation, we also implemented a N-ary tree summation in the same variations as that of binary.

\begin{listing}
    \begin{minted}{csharp}
public static int DoFakeWork(int workBias) {
    //Start the 'work bias' before blocking wait on child computation, i.e. we're waiting while the children
    //are computing
    var fakeSum = 0;
    for(var i = 0; i < workBias / 2; i++) {
        fakeSum += i;
    }
    for (var i = 0; i < workBias / 2; i++) {
        fakeSum -= i;
    }
    return fakeSum;
}

public static async Task<int> SumLeaves(Tree<int> tree, int workBias)
{
    if (tree is Leaf<int> leaf)
        return leaf.Value;

    var sums = tree.Children.Select(c => SumLeaves(c, workBias)).ToList();
    
#if !DELAY_DEPENDS_ON_LR
    var workBias = Task.Run(() => DoFakeWork(workBias));
    await Task.WhenAll(sums);
    var fakeSum = await workBias;
#else
    await Task.WhenAll(sums);
    var fakeSum = DoFakeWork(workBias);
#endif
    
    return sums.Sum(t => t.Result) + fakeSum;
}
    \end{minted}
    \caption{Implementation of the two different data dependency strategies with an N-ary tree. The strategy may be selected by either defining or undefining the \ttt{DELAY\_DEPENDS\_ON\_LR} preprocessor flag.}
    \label{lst:benchmark:strategies}
\end{listing}

\subsubsection{Results}
The results are plotted in \figureref{crit-work-dep} and \figureref{crit-work-no-dep} (and listed in \tableref{binary:tree:with:bias:dependency} and \tableref{binary:tree:with:bias:no:dependency} in \apxref{crit:work:data}).

\newcommand{\workBiasSymbolics}{\symbolic{Work Bias (iterations),134217728,67108864,33554432,16777216,8388608,4194304,2097152,1048576,524288,262144,131072,65536,32768,16384,8192,4096,2048,1024}}
\lineChart[][Run Time (ns)][Number of Busy-Wait Iterations]{Critical workload with data dependency.}{crit-work-dep}{
    \plotData{Sequential}{\workWithDependencyData}
    \plotData{Fork Join}{\workWithDependencyData}
    \plotData{Lenient}{\workWithDependencyData}
}
\lineChart[][Run Time (ns)][Number of Busy-Wait Iterations]{Critical workload without data dependency.}{crit-work-no-dep}{
    \plotData{Sequential}{\workWithoutDependencyData}
    \plotData{Fork Join}{\workWithoutDependencyData}
    \plotData{Lenient}{\workWithoutDependencyData}
}
\lineChart[][Run Time (ns)][Number of Busy-Wait Iterations]{Critical workload with data dependency, N-ary tree.}{crit-work-dep-nary}{
    \plotData{Sequential}{\workWithDependencyDataNary}
    \plotData{Fork Join}{\workWithDependencyDataNary}
    \plotData{Lenient}{\workWithDependencyDataNary}
}
\lineChart[][Run Time (ns)][Number of Busy-Wait Iterations]{Critical workload with data dependency, N-ary tree.}{crit-work-no-dep-nary}{
    \plotData{Sequential}{\workWithoutDependencyDataNary}
    \plotData{Fork Join}{\workWithoutDependencyDataNary}
    \plotData{Lenient}{\workWithoutDependencyDataNary}
}

\makeTable{
    { c | c | c }
    & No data dependency & Data dependency \\\hline
    Binary & 1024 & 4096 \\
    N-ary & 2048 & 2048
}{Iterations of the busy-wait loop before the sequantial solution becomes the fastest.}{crit:work:iterations}

The number of iterations in the busy-wait loop before the sequential solution is faster than the concurrent is listed in \tableref{crit:work:iterations}. The graphs also underline that our hypothesis was correct. The parallel solutions grows slower than the sequential because they are capable of executing multiple busy-waits concurrently. Finally we notice that the lenient and fork join strategy lie very close in execution speed. This is a promising result for the lenient evaluation strategy, as it shows that it may be as fast as a traditional concurrency strategy.

Some concurrency models batch smaller jobs together to form larger jobs. Such batching may reduce the time spent context switching and thus increase the execution speed of the concurrent solutions. Such strategy is employed by Unity's C\# job system\cite{unity:csharp:job:system}.

\subsection{Matrix Summation}
In this section we execute a matrix summation benchmark. This benchmark measures the time it takes to sum all indices of a random $N x N$ matrix. This benchmark was implemented in both C\# and F\# with four different parallelisation strategies to explore how well they scale to increasing sizes of $N$:

\begin{labeling}{\quad\quad}
    \item[Sequential] utilises a double-nested for-loop to iterate over the matrix and sum the values. This benchmark provides a baseline value for running the computation on one thread.
    \item[Map Reduce] maps a function that sums each column over the matrix. The resulting list of column sums is then reduced to the overall sum of the matrix. In C\# we utilise the \gls{LINQ}-methods \ttt{Select}, \ttt{Sum} and \ttt{Aggregate} and in F\# we use \ttt{List.map} and \ttt{List.reduce}.
    \item[Parallel Foreach] uses a parallel loop to iterate over the columns of the matrix that may execute the summation of each column in parallel.
    \item[Tasks] is similar to parallel foreach, with the only exception that we manually spawn a \ttt{Task} that calculates the sum of each column.
\end{labeling}

We have not included a lenient variation in this experiment, as an implementation in our mapping would be largely equivalent to the tasks-implementation. The most notable difference being that a lenient-evaluation strategy would most likely also construct the matrix in parallel with the summation. As the time it takes to construct a matrix is not included in the results here, this should have no effect on the validity of the results.

As the matrices are of size $N x N$, they contain a total of $N^2$ elements, which are initialised with random values between \ttt{Int64.Minvalue} and \ttt{Int64.MaxValue}. When running the test with large matrices we found that the result would overflow, which throws an exception because C\# and F\# are a managed languages. In order to avoid this, we used the \ttt{unchecked}-keyword in C\#, which disables bounds-checking on an integral arithmetic operation\cite{csharp:unchecked}. \cite{csharp:unchecked} states that using \ttt{unchecked} \textit{\dquote{might improve performance}}, compared to checked integral arithmetic operations. The \ttt{unchecked} keyword does not exist in F\#, instead we found a StackOverflow post the stated that implementing a custom operator would be our best choice (\fsline{let (+!) (x:int64) (y:int64) = Operators.(+) x y})\cite{fsharp:unchecked}.

\subsubsection{Results}
The results are plotted in \figureref{linpack-summation-fs} and \figref{linpack-summation-cs}. The first thing to notice is that Map Reduce seems to be roughly equal to the sequential in running time. This could indicate that the \ttt{Select}-method of C\#'s \gls{LINQ}, which was used to implement Map Reduce, does not parallelise its iterations. We will thus treat Map Reduce as a sequential solution for the rest of this result discussion.

\newcommand{\linpackSymbolics}{\symbolic{Problem Size,2,4,8,16,32,64,128,256,512,1024,2048,4096}}
\barChart*[7][\linpackSymbolics][Run Time (ns)][Size of matrix colums and rows]{Matrix Summation in F\#}{linpack-summation-fs}{
    \plotDataWithError{Sequential}{\matrixSumData}
    \plotDataWithError{Map Reduce}{\matrixSumData}
    \plotDataWithError{Parallel Foreach}{\matrixSumData}
    \plotDataWithError{Tasks}{\matrixSumData}
}
\barChart*[7][\linpackSymbolics][Run Time (ns)][Size of matrix colums and rows]{Matrix Summation in C\#}{linpack-summation-cs}{
    \plotDataWithError{Sequential}{\matrixSumDataCsharp}
    \plotDataWithError{Map Reduce}{\matrixSumDataCsharp}
    \plotDataWithError{Parallel Foreach}{\matrixSumDataCsharp}
    \plotDataWithError{Tasks}{\matrixSumDataCsharp}
}
In general, the results from this experiment is in alignment with those of the previous, in that there is an initial overhead associated with concurrency. In this case, it seems the sequential and concurrent solutions evens out at job sizes of around 512 summations, after which point the concurrent solutions are faster.

After overcoming the initial overhead, the concurrent solutions handle increasing matrix sizes much better than their sequential counterparts. This is even more notable in \figureref{linpack-summation-line}, which plots the same data as a line and without logarithmic y-axis. As the matrix sizes continue to grow, it may be possible to split the columns in multiple separate tasks, possibly making the concurrent implementations faster yet.

\lineChart[][Run Time (ns)][Size of matrix Columns and Rows]{Matrix Summation}{linpack-summation-line}{
    \plotData{Sequential}{\matrixSumData}
    \plotData{Map Reduce}{\matrixSumData}
    \plotData{Parallel Foreach}{\matrixSumData}
    \plotData{Tasks}{\matrixSumData}
}

In \figref{linpack-summation-cs-vs-fs} we have plotted the concurrent implementations in C\# and F\#. This plot shows that the Task implementation in F\# is faster with smaller problem sizes. It remains the fastest up until N is around 128. After that point the C\# Task implementation becomes the fastest and continues to remain so. It seems to scale better with problem sizes than the other implementations. The parallel foreach implementations lie around the same running time, meaning that the languages are equally performant with that concurrency strategy.
\barChart*[7][\linpackSymbolics][Run Time (ns)][Size of matrix colums and rows]{Matrix Summation in C\# compared to F\#}{linpack-summation-cs-vs-fs}{
    \plotDataWithErrorAndLegend{Parallel Foreach}{\matrixSumDataCsharp}{C\# Parallel Foreach}
    \plotDataWithErrorAndLegend{Tasks}{\matrixSumDataCsharp}{C\# Tasks}
    \plotDataWithErrorAndLegend{Parallel Foreach}{\matrixSumData}{F\# Parallel Foreach}
    \plotDataWithErrorAndLegend{Tasks}{\matrixSumData}{F\# Tasks}
}

\subsection{Discussion}
Our results show, first and foremost, that there is an initial overhead in concurrent programming. In order to speed up a computation the task sizes must exceed a certain threshold. Our results show that this threshold lies around the size of 1024-4096 iterations in a for-loop that increments a variable\tmc{Måske skal vi tælle nogle instruktioner her?}, but also that it varies with the type of problem. Furthermore, the results show that the concurrency of lenient evaluation strategy in these cases is similar to that of classic concurrency strategies, but that the choice of problems in \cite{DBLP:journals/cl/Tremblay-parallel} is not well suited to parallelisation. Finally C\#'s \ttt{Task}-model seems to handle the matrix summation problem well, after exceeding the threshold of roughly 512 summations in each column.

As part of the project we implemented a simple \gls{FRP} system in Unity, because this particular programming paradigm is well suited to gameplay programming in functional languages\cite{maraffi:frp,cheong2005functional,courtney2003yampa}. We had initially decided that this experiment would use either lenient evaluation or Async Workflows under the hood to update \ttt{MonoBehaviour}s concurrently, but discovered too late that Unity uses a custom concurrency strategy called Unity C\# Job System\cite{unity:csharp:job:system} (see \secref{unity:concurrency}). Unity therefore does not allow \ttt{MonoBehaviour} updates from \ttt{Task}s and Async Workflows\cite{unity:async}. The resulting \gls{FRP} system therefore runs sequentially.