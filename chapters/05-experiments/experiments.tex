\chapter{Concurrency in C\#, F\# and Unity}
In the previous section we uncovered that even though experienced gameplay programmers wrote more concise code in F\#, they were reluctant to switch. In order to introduce functional gameplay programming in the industry, we therefore need stronger incentive. One such incentive could be implicit (or at least simpler) concurrency. In \cite{DBLP:journals/cl/Tremblay-parallel} the author argues that the lenient evaluation strategy, which was also proposed by Sweeney, provides high implicit parallelisability. This evaluation strategy may be implemented in the \gls{FRP} system that we prototyped during the usability test.

In this section we research how the lenient evaluation strategy compares to classic concurrency strategies. During this research we found that there is a certain threshold of task-sizes, which must be exceeded before concurrency has a positive effect on execution time. We attempt to estimate this threshold in \secref{crit:work} using two tests; a busy-wait delay estimation and matrix summation.

In the final section of this chapter we measure the performance of the \gls{FRP} system to ascertain whether or not the use of functional programming in Unity results in a performance penalty. 

\input{chapters/05-experiments/benchmarks.tex}
\input{chapters/05-experiments/crit-work.tex}
\input{chapters/05-experiments/performance.tex}

\section{Threats to Validity}
In this section we examine threats to validity in the performance benchmarks that were presented in this chapter. Once again we use the methodology proposed by \cite{mcleod:validity}. The points discussed here are in general applicable to both the concurrency benchmarks, the critical work benchmarks and the Unity \gls{FRP} benchmarks.

\subsection{Internal Threats}
In this section we discuss internal threats. Internal threats may originate from the test cases, execution and data analysis. We both discuss the origin of the threats as well as what we did to counteract it.

\subsubsection{Experience}
One of the threats to validity is our experience with F\#. None of us had written F\# before starting this project and thus our experience is in the vicinity of months. We have spent time examining best practices and how to implement concurrent code in F\#, but some programming tasks come more easily with doing rather than reading. Furthermore, it is well known that writing concurrent code is notoriously difficult and requires greater experience\cite{nanz2013benchmarking,nanz2013examining}. 

\subsubsection{Test Cases}
In the F\# concurrency benchmark we reused the problems presented in \cite{DBLP:journals/cl/Tremblay-parallel}, as the author claimed that they were well suited for the lenient evaluation strategy. We discovered that it was not the case. In order to research how the lenient evaluation strategy fares when the amount of work grows, we introduced a modified version of the binary summation benchmark, which emulates work in each node. We also introduced a matrix summation test, where each column was summed concurrently. These tests allow only a rather limited peek into how well F\# parallelises in comparison to C\#, as they are very specialised problems. Furthermore, they do not provide any information about how the two languages parallelise in the context of a game, as we argue that it is not common to sum large matrices and accumulate leaves of trees in games.

\subsection{External Threats}
In this section we examine external threats. External threats are those that arise outside the testing environment and may affect how broadly applicable the test results are. We first present the origin on the threat and then outline how the tests could be altered in the future to provide more broadly applicable results.

\subsubsection{Limited Generalisability}
All the benchmarks were run on a single test machine, as listed in \tableref{sys-specs}. This means that the results may have limited generalisability to other system setups. This threat can rather easily be counteracted by running the benchmarks on other computers. The \gls{CPU} of the test machine was announced on the 6th of April 2013\cite{i7:specs}, meaning that it is a little over six years old at the time of writing. It would therefore be beneficial to rerun the benchmarks on a newer processor to obtain more contemporary results. 