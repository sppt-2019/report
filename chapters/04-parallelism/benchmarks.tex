\section{Benchmarks}
In this section we explore the performance of the lenient evaluation strategy and how well it parallelises. The reason for this is that the interest in it has been lacking to say the least. We could not find a reason as to why and decided to explore whether it was because \cite{DBLP:journals/cl/Tremblay-parallel} gave false promises.

\subsection{Test cases}
\cite{DBLP:journals/cl/Tremblay-parallel} presents two test cases that we reuse in this experiment:

\begin{labeling}{\quad\quad}
    \item[Binary Tree Sum] where a tree-walker sums the values of all leaves of a binary tree.
    \item[Binary Tree Accumulation and Sort] where a tree-walker flattens all values of a tree's leaves into a list and sorts it.
\end{labeling}

The difference between the two test cases is that there are no data-dependencies in the summation test, i.e. we can calculate the results of the left subtree and the right subtree in parallel. In the Binary Tree Accumulation and Sort benchmark the tree must be traversed in a left-to-right order and thus there is a data-dependency between the results from the left and right subtree.

\subsection{Implementations}
In this benchmark we decided to use C\#, as we were familiar with the language. Furthermore, we discovered in previous work that C\#'s performance in the .NET Core runtime in many cases was faster than that of C++ in the context of microbenchmarks \cite{p92018gameplay}. We are well aware that C\# is not lenient, but we wanted to compare the strategies within the same language runtime and therefore attempted to map the lenient evaluation strategy onto C\#'s task system. \lstref{lenient:to:task} gives a code example how to do so.

\begin{listing}[H]
\begin{minted}{csharp}
//Lenient function, body evaluated asynchronously from caller
//parameters evaluated asynchronously from function body
int CalculateC(int a, int b) {
    int c = a * 50; //Implicit synchronisation with a-thread
    return c + b;   //Implicit synchronisation with b-thread
}

//C# mapping of the CalculateC-function
//async method that runs asyncronously from the caller
async Task<int> CalculateC_Lenient(Task<int> a, Task<int> b) {
    int c = await a * 50;   //Explicit synchronisation with a-task
    return c + await b;     //Explicit synchronisation with b-task
}
\end{minted}
\caption{Lenient evaluation in C\#} \label{lst:lenient:to:task}
\end{listing}
The mapping shown in \lstref{lenient:to:task} involves wrapping all arguments to methods in \ttt{Task}s and explicitly synchronise two \ttt{Task}s whenever there is a data-dependency between them. This means that all values of arguments are calculated asynchronously from the method body. The method is marked as \ttt{async}, meaning that a \ttt{Task} is spawned for every invocation of the method, i.e. it runs asyncronously from the caller. This leaves the majority of the footwork to the .NET Core task scheduler, which must figure out in which order to execute them.

We implemented the test cases in three different synchronisation models:
\begin{labeling}{\quad\quad}
    \item[Sequential] which uses divide and conquer on one unit of execution to do all the calculations. This provides the baseline speed for the computation on a single core.
    \item[Fork-Join] which also uses divide and conquer, but in each recursion step, two \ttt{Task}s are spawned to compute the results of the left and right subtree. The \ttt{Task}s are then synchronised at the end of each recursive call. This provides a \textit{\dquote{classic parallel}} baseline speed.
    \item[Lenient] using the mapping displayed in \lstref{lenient:to:task}, i.e. wrapping everything in \ttt{Task}s and have the .NET Core task scheduler figure out the computation order.
\end{labeling}

\subsection{Test Setup}
According to \cite{sestoft2013microbenchmarks} the most reliable results are obtained when the tests are repeated multiple times and the average and standard deviation calculated for the results. We therefore decided to take the average execution speed over 100 repetitions for varying problem sizes, starting with 10 leaf nodes and gradually increasing the number up to a total of 10,000 leaves.

The tests were run on a laptop, of which the specifications are listed in \tableref{sys-specs}.

\makeTable{
{| l | R{6em} | p{3em} |}
\hline
\multicolumn{3}{| c |}{\textbf{Processor}} \\ \hline
Model & \multicolumn{2}{| c |}{Intel Core i7 4702HQ} \\ \hline
Clock Frequency & 2.2 & GHz \\ \hline
Max Turbo & 3.2 & GHz \\ \hline
Physical & 4 & Cores \\ \hline
Logical\footnotemark & 8 & Cores \\ \hline
\multicolumn{3}{|c|}{\textbf{Memory}} \\ \hline
Memory Size & 16 & GiB  \\ \hline
Memory Speed & 1600 & MHz \\ \hline
Memory Type &  \multicolumn{2}{| c |}{DDR3L 1600} \\ \hline
\multicolumn{3}{|c|}{\textbf{Software}} \\ \hline
Operating System & \multicolumn{2}{| c |}{Ubuntu 18.04 64bit}  \\ \hline
C\# runtime & \multicolumn{2}{| c |}{dotnet 2.2.104} \\ \hline
}{System specifications.}{sys-specs}\footnotetext{Logical cores are sometimes called threads. However logical cores is used here to avoid confusion with the software concept; threads, which is distinct from hardware threads.}

\subsection{Results}
The results are plotted in \figref{binary-accumulation} and \figref{binary-summation} (and listed in \tableref{accumulation:res} and \tableref{summation:res} in \apxref{benchmark:data}).

\newcommand{\binAcumSymbolics}{\symbolic{Problem,10,100,1000,10000,100000}}
\barChart[15][\binAcumSymbolics]{Binary Accumulation}{binary-accumulation}{
    \plotDataWithError{Sequential}{\accumulationData}
    \plotDataWithError{Fork Join}{\accumulationData}
    \plotDataWithError{Lenient}{\accumulationData}
}\btc{?? -in reference to problem size 100}
\barChart[15][\binAcumSymbolics]{Binary Summation}{binary-summation}{
    \plotDataWithError{Sequential}{\summationData}
    \plotDataWithError{Fork Join}{\summationData}
    \plotDataWithError{Lenient}{\summationData}
}

Much to our surprise, the sequential implementation was actually the fastest in all cases. It seems that the overhead of spawning and synchronising \ttt{Task}s outweighs the performance gain of parallelisation, when the problem sizes are in the magnitude of additions and list appending. Furthermore, the Accumulation test case presented in \cite{DBLP:journals/cl/Tremblay-parallel} is a poor choice when it comes to parallelism, as it must traverse the tree in a left-to-right manner, meaning that the only things that can be executed in parallel is the recursive calls down the tree. Another interesting result is that the execution time of the lenient approach grows much faster with the problem size compared to both Fork Join and Sequential. We suspect that the advantages of parallel programming will be more prominent, as the amount of work in each unit of execution increases. We will research this suspicion in greater depth in the following section.
