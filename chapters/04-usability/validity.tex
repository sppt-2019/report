\subsection{Threats to Validity} \label{sec:validity}
In this section we will examine potential sources of errors and other threats to validity. Such threats are categorised as either internal or external. An internal threat occurs when data is mishandled, misinterpreted or in some other way skewed to such a degree that the results are untrustworthy. The second category consists of errors caused by the data being inapplicable to other cases. This is inline with terminology outlined in \cite{mcleod:validity}.


\subsubsection{Internal Validity}
In this section the internal validity is explored. The goal of this section is to map out the possible shortcomings originating from data handling and interpretation. Each potential threat is explained in turn and actions undertaken to mediate the threat are outlined. In some cases a threat cannot be sufficiently mediated, in which case it is simply listed.

\paragraph{Evaluation Parameters}
The user tests were evaluated in accordance with the \champagne method (see \secref{champagne}). This methodology is intended to be used to measure the usability of a single feature, not an entire programming language. Therefore it was modified to fit our case better. The methodology consists of two other usability techniques: \cognitive and \attentions (see \secref{cog-dim} and \secref{attention-investment}). We modified the \cognitive aspect to focus on the languages as a whole and kept the feature focus of \attention. Thus the evaluation parameters were centered on the users' experience with \fs and their understanding of the \gls{FRP} system.

The \cognitive framework is originally intended to estimate the usability, or provide a vocabulary to such an estimation, of a notational system such as a programming language. Therefore our modification of \champagne is to use \cognitive for its original purpose. Furthermore, a single aspect of \discount was used, namely the sample sheet (see \secref{discount-method}). This was employed to assist test participants with \fs.

\paragraph{Task Difficulty}
The user test consisted of a number of tasks that participants were asked to complete. These tasks were inspired by game development scenarios and applicability of functional idioms. Before the test we were worried that some tests were more difficult than others. This could skew the results, as participants completing the more difficult tasks would struggle more than participants completing the easier tasks. The dialogue tree task (see \secref{usability:test:cases}) presented a much more difficult problem than expected. The task relied on recursion and tree structures which were unfamiliar to the participants.

After the user test it became apparent that some tasks were indeed more difficult than others, significantly so in some cases. The difficulty of the tasks themselves did not affect the results to the degree we had expected, instead the difficult tasks brought conflicting idioms and faulty problem solving approaches to light that may not have been as apparent in the easier tasks.

\paragraph{Task Presentation}
During the tests it became apparent that some tasks were not formulated clearly enough. This meant that participants misunderstood the tasks and therefore did not sufficiently complete the task. These misunderstandings came from unclear task descriptions and lacking context. To mediate this we conducted a pilot test. This test clarified several unclear task descriptions before the user test. Unfortunately this test did not catch all the obscurities in the tasks.
\tmc{Det lyder underligt at vi skriver at vi under testen fandt ud af at de var dårligt formuleret og så afholdte en pilottest. Det er lidt omvendt.}

Furthermore, some tasks could have been constructed better. An example of this is the dialogue tree task. Instead of constructing a class that participants would use, an interface could have been formulated. This would have implicitly clarified the purpose of the class. \tmcc{This section is a bit sketchy}

\paragraph{Sample Size \& Test Duration}
The user test was conducted with six participants outlined in \secref{par-crit}. This was inline with the \champagne methodology. In order to analyse the test results qualitative methods were employed. According to \cite{virzi1992refining} six participants should allow us to discover 80\% of the usability issues. However, there is no consensus on the optimum number of participants for such a usability study\cite{hwang2010number}. However, the participants only had an hour to complete exercises in both \fs and \cs. This meant that most participants only managed to complete a single test case in each language, which provides only limited insight into the usability of the languages. Some test cases focused on certain aspects of the programming language's idiom, such as recursion, which some participants were unfamiliar or inexperienced with.

These problems could be mediated by allocating more time for the tests, however taking up an hour of the participants' day, presents scheduling issues already. Therefore, extending the tests was not an option. In order to mediate this issue the test could be restructured so that the participants worked solely with \fs. This would provide more information about the use and challenges of F\#, but not allow us to compare the F\# and C\# code.

\subsubsection{External Validity}
This section explores the potential threats originating from outside the test itself. This means any threat that can affect or skew the results that is not directly related to the test conduction. An example of this is whether the test tests the right things and whether the findings are broadly applicable or not.

\paragraph{Applicability to Game Development}
The test cases were designed to mimic gameplay programming scenarios. These cases were constructed to showcase situations where conventional and functional solution strategies could compete. However, the realism of the these cases is questionable. Our knowledge of the game development industry stems from a literature study, which need not align with reality.

Some participants commented on the realism of the test cases. One participant noted that the character controller tasks were not representative of real development as they are freely available, and even if they weren't the developer would buy one from the asset store. Other participants indicated that they had spent a lot of time writing character controllers.

\paragraph{Development Environment \& Test Setup}
In order to use \fs with \unity a custom plugin was used. This plugin was developed by another student on the Programming Technology specialisation at Aalborg University\cite{fsharp2019plugin}. \unity does not officially support \fs and in order to run the \fs code a work around was employed. The plugin automated this process and allowed the test participants to focus on the task at hand. However, the use of the plugin presented some problems. Compilation of \fs code was done in the \unity editor instead of the \gls{IDE}. These problems were mediated along the way, by reporting them to the developer. The plugin was patched repeatedly during test construction, but no plugin errors were encountered during the test.

Some errors were encountered during the test, however these errors originated from the setup code. Some errors had been identified and fixed as a result of running the pilot test. Unfortunately full test coverage was not achieved during the pilot test and some errors remained undiscovered until the test. This could have been mediated with additional pilot testing or unit testing to verify that changes did not break the scenes.
