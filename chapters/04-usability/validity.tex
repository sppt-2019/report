\discount\subsection{Threats to Validity} \label{sec:validity}
In this section we will examine potential sources or errors and other threats to validity. Such threats are categorised as either an internal threat or an external threat. An internal threat occurs when data is mishandled, misinterpreted or in some other way skewed to such a degree that the results are untrustworthy. The second category consists of errors caused by the data being inapplicable to other cases. This inline with terminology outlined in \cite{mcleod:validity}.


\subsubsection{Internal Validity}
In this section the internal validity is explored. The goal of this section is map out the possible  shortcomings originating from data handling and interpretation. Each potential threat is explained in turn and actions undertaken to mediate the threat are outlined. In some cases a threat cannot be sufficiently mediated, in which case it is simply listed.

\paragraph{Evaluation Parameters}
The user tests where evaluated in accordance with the \champagne method (see \secref{champagne}). This methodology is intended to be used to measure the usability of a single feature, not an entire programming language. Therefore it was modified to fit our case better. The methodology consists of two other usability techniques: \cognitive and \attentions (see \secref{cog-dim} and \secref{attention-investment}). We modified the \cognitive aspect to focus on the languages as a whole and kept the feature focus of \attention. Thus the evaluation parameters where centered on the users experience with \fs and their understanding of the \gls{FRP} system.

The \cognitive framework is originally intended to  estimate the usability, or provide vocabulary to such an estimation, of a notational system such as a programming language. Therefore our modification of \champagne is to use \cognitive for its original purpose. Furthermore, a single aspect of \discount was used, namely the sample sheet (see \secref{discount-method}). This was employed to assist test participants with \fs.

\paragraph{Task Difficulty}
The user test consisted of a number tasks that participants where asked to complete. These tasks where inspired by game development scenarios and applicability of functional idioms. Before the test we where worried that some tests where more difficult than others. This could skew the results somewhat since, participants completing the more difficult tasks would struggle more then participants completing the easier tasks. The dialogue tree task (see \secref{usability:test:cases}) presented a much more difficult problem than expected. The task relied on recursion and tree structures which where unfamiliar to participants.

After the user test is became apparent that some tasks where indeed more difficult than others, significantly so in some cases. The difficulty of the tasks themselves did not affect the results to the degree we had expected, instead the difficult tasks brought conflicting idioms and faulty problem solving approaches to light that may not have been as apparent in the easier tasks.

\paragraph{Task Presentation}
During the tests it became apparent that some tasks where not formulated clearly enough. This meant that participants misunderstood the task and therefore did not sufficiently complete the task. These misunderstandings came from unclear task descriptions and lacking context. To mediate this we conducted a pilot test with another software student. Using this test, clarified several tasks before the user test. Unfortunately this test did not catch all the obscurities in the tasks.

Furthermore, some tasks could have been constructed better. An example of this is the dialogue tree task, instead of constructing a class that participants would use, an interface could have been formulated. This would have implicitly clarified the purpose of the class. \tmcc{This section is a bit sketchy}

\paragraph{Sample Size \& Test Duration}
The user test was conducted with six participants outlined in \secref{par-crit}, this was inline with the \champagne methodology. In order to analyse the test results qualitative methods where employed, therefore the number of participants is sufficient to find 80\% of usability issues\cite{virzi1992refining}. However, there is no consensus on the optimum number of participants for such a usability study\cite{hwang2010number}. However, the participants only had an hour to complete exercises in both \fs and \cs. This meant that most participants only managed to complete a single test case in each language, which provides limited insight into the programming ability of the participant. Some test cases focus on certain aspects of the programming language's idiom, such as recursion, which some participants where unfamiliar or inexperienced with.

These problems could be mediated by allocating more time to the tests, however taking up an hour of the participants day, presents scheduling issues already. Therefore extending the tests where not an option. In order to mediate this issue somewhat the test restructured so that participants worked with \fs in 40 minutes and \cs in 20 minutes, based on the pilot test. The assumption was that the participants where experienced in \cs and new to \fs. \tmcc{Weak arguments, reevaluate later}


\subsubsection{External Validity}
This section explores the potential threats originating from outside the test itself. This means any threat that affect or skew the results that is not directly related to the test conduction. An example of this is whether the test tests the right things and whether the findings are broadly applicable or not.

\paragraph{Applicability to Game Development}
The test cases, the tasks participants where asked complete, where designed to mimic game development scenarios. These cases where constructed to showcase situations where conventional and functional solution strategies could compete. However, the realism of the these cases is questionable. We do not have industrial game development experience and therefore our experiences may not represent the reality.

Some participants commented on the realism of the test cases. One participant noted that the character controller tasks where not representative of real development since they are freely available and even if they weren't the developer would buy one from the asset store. The other participants indicated that they had spent a lot of time writing character controllers.

\paragraph{Development Environment \& Test Setup}
In order to use \fs with \unity a custom plugin was used. This plugin was developed by a student at Aalborg University\cite{fsharp2019plugin}. \unity does not officially support \fs and in order to run the \fs code a work around was employed. The plugin automated this process and allowed the test participants to focus on the task at hand. However, the plugin was not without problems. Compilation of \fs code was done in the \unity editor instead of the \gls{IDE}. These problems where mediates along the way, by reporting them to the developer. The plugin was patched repeatedly during test construction and no plugin errors where encountered during the test.

Some errors where encountered during the test, however these errors originated from the setup code. Some errors had been identified and fixed as a result of running the pilot test. Unfortunately full test coverage was not achieved during the pilot test and some errors remained undiscovered until the test. This could have been mediated with additional pilot testing or a simulating all tests to verify changes did not break them.
